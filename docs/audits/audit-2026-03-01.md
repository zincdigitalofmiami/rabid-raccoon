# Codebase Audit — 2026-03-01
**Scope:** Full read-only codebase review  
**Auditor:** GitHub Copilot Agent  
**Status:** FINDINGS — Action required before next production deploy

---

## TABLE OF CONTENTS
1. [Security Findings](#security-findings)
2. [Architecture & Design Findings](#architecture--design-findings)
3. [Incomplete / Stub Code](#incomplete--stub-code)
4. [Data Quality & Consistency](#data-quality--consistency)
5. [Operational Risks](#operational-risks)
6. [Positive Patterns](#positive-patterns)
7. [Priority Summary](#priority-summary)

---

## SECURITY FINDINGS

### S-1 (P0): No Authentication on Any API Route
**Files:** All `src/app/api/*/route.ts`

Every API endpoint is publicly accessible with zero authentication:
- `/api/analyse/ai` — calls OpenAI at ~$0.05–$0.20 per request (uncapped)
- `/api/analyse/chart` — calls Anthropic Claude with user-supplied image data
- `/api/trades/upcoming` — calls OpenAI per triggered setup
- `/api/news/scrape` and `/api/news/scrape-reports` — write to the database
- `/api/ingest/econ-calendar` — long-running DB write operation (maxDuration 300s)
- `/api/live/mes` and `/api/live/mes15m` — open persistent SSE streams

There is no middleware, no API key header check, no IP allowlist, and no `CRON_SECRET` pattern. Any external caller with the Vercel URL can exhaust API quotas or consume AI credits.

**Risk:** Financial (AI cost abuse), data integrity (writes via ingest routes), DoS (SSE stream exhaustion).

**Recommendation:** Add a shared secret check to all routes that trigger AI calls or DB writes. Vercel deployment with no custom domain may offer some protection via obscurity, but this is not a security control.

---

### S-2 (P1): No Image Size or Content Validation on `/api/analyse/chart`
**File:** `src/app/api/analyse/chart/route.ts:82–115`

The chart analysis endpoint accepts an arbitrary `body.image` base64 string and forwards it directly to Anthropic. There is:
- No size limit check (Next.js default body limit is 4MB, but large PNGs encoded as base64 can approach this)
- No content type verification (only a string replace of the prefix, not cryptographic check)
- No check that the string is valid base64

```typescript
// Current — no size check before Anthropic call
const base64Data = body.image.replace(/^data:image\/\w+;base64,/, '')
const client = new Anthropic({ apiKey })
const response = await client.messages.create({ ... data: base64Data ... })
```

**Recommendation:** Add a size check (`body.image.length > MAX_BASE64_SIZE`) and validate that the base64 string is well-formed before calling Anthropic.

---

### S-3 (P1): `$queryRawUnsafe` Without Allowlist Validation in Dataset Builders
**Files:** `scripts/build-15m-dataset.ts:302`, `scripts/build-lean-dataset.ts:907`

Both dataset scripts use `$queryRawUnsafe` with table names interpolated from `FRED_FEATURES` constant:

```typescript
const rows = await prisma.$queryRawUnsafe<...>(
  `SELECT "seriesId", ... FROM "${table}" WHERE ...`,
  seriesIds
)
```

The table names come from the internal `FRED_FEATURES` constant (not user input), so the _actual injection risk is low_. However, unlike the security-conscious sibling scripts (`_staleness-audit.ts:6`, `_check-econ-tables.ts:6`) that both have explicit `ALLOWED_TABLES` allowlists and `assertAllowedId()` guards, these scripts lack the defense-in-depth pattern.

**Recommendation:** Add the same `ALLOWED_TABLES`/`assertAllowedId()` pattern from `_staleness-audit.ts` to `build-15m-dataset.ts` and `build-lean-dataset.ts` for consistency and defense-in-depth.

---

### S-4 (P2): AI Response Parsing Without Schema Validation
**Files:** `src/lib/forecast.ts`, `src/lib/instant-analysis.ts`, `src/lib/trade-reasoning.ts`

AI responses from OpenAI/Anthropic are parsed with `JSON.parse` and cast directly to application types:

```typescript
// In instant-analysis.ts
return JSON.parse(text) as AnalysisAiResponse
// In forecast.ts
const parsed = JSON.parse(textContent) as ForecastResponse
// In trade-reasoning.ts
const parsed = JSON.parse(content) as { adjustedPTp1?: number; ... }
```

While the AI guardrails in `trade-reasoning.ts` do clamp probability values post-parse, there's no structural schema validation. A malformed AI response with injected properties or wrong types would pass through.

**Recommendation:** Add runtime shape validation (e.g., check that required number fields are `typeof x === 'number'`) before returning parsed AI data to the client. This is especially important for `trade-reasoning.ts` since its output directly influences financial decision support.

---

## ARCHITECTURE & DESIGN FINDINGS

### A-1 (P1): Hardcoded Symbol List in `backfill-futures-all.ts`
**File:** `scripts/backfill-futures-all.ts:28–32`

```typescript
const NON_MES_ACTIVE = [
  'ES', 'NQ', 'YM', 'RTY', 'SOX',
  'ZN', 'ZB', 'ZF', 'ZT',
  'CL', 'GC', 'SI', 'NG',
  '6E', '6J', 'SR3', 'SR1', 'ZQ', 'MNQ', 'MYM',
]
```

This directly violates AGENTS.md Rule #1 ("No Hardcoded Symbol Lists"). The list should be sourced from the symbol registry using `getSymbolsByRole()` with an appropriate role key (e.g., `INGESTION_ACTIVE` filtered to non-MES).

---

### A-2 (P1): `EXPECTED_SYMBOLS` Hardcoded in Correlation Route
**File:** `src/app/api/mes/correlation/route.ts:32`

```typescript
const EXPECTED_SYMBOLS = ['MES', 'NQ', 'VX', 'DX'] as const
```

Minor violation of AGENTS.md Rule #1. The "expected symbols for correlation" set should be defined as a role in the registry (e.g., `CORRELATION_SET`) and queried at runtime.

---

### A-3 (P1): Model Hardcoded in `/api/analyse/chart` — No Fallback
**File:** `src/app/api/analyse/chart/route.ts:117`

```typescript
const response = await client.messages.create({
  model: 'claude-opus-4-6',
  ...
})
```

Unlike every other AI call in the codebase which uses a model cascade (`OPENAI_ANALYSIS_MODEL` env var → list of fallback models), this endpoint hardcodes `claude-opus-4-6` with no environment variable override and no fallback. If this model is unavailable, the endpoint fails with no recovery path.

**Recommendation:** Use `process.env.ANTHROPIC_ANALYSIS_MODEL || 'claude-opus-4-6'` and add a fallback model in a try/catch.

---

### A-4 (P2): Duplicate `rowToCandle` / `prismaRowToCandle` Helper
**Files:**
- `src/app/api/analyse/trades/route.ts:35`
- `src/app/api/mes/correlation/route.ts:38`
- `src/app/api/mes/setups/route.ts:32`
- `src/app/api/trades/upcoming/route.ts:61`
- `src/app/api/live/mes/route.ts:39`
- `src/app/api/live/mes15m/route.ts` (different name but identical logic)
- `src/app/api/analyse/chart/route.ts:43`

The same 8-line function converting Prisma decimal rows to `CandleData` is copy-pasted across 7+ files. Any change to the conversion logic (e.g., handling edge cases) must be applied in all locations.

**Recommendation:** Extract to `src/lib/prisma-to-candle.ts` and import from there.

---

### A-5 (P2): Duplicate `aggregateCandles` Function
**Files:** `src/app/api/market-data/batch/route.ts:10`, `src/app/api/forecast/route.ts:13`, `src/lib/analyse-data.ts`

Three independent implementations of the same OHLCV bar aggregation logic. Should be the single version in `src/lib/analyse-data.ts` (already exported as `aggregateCandles`) imported by the route files.

---

### A-6 (P2): `BhgPhase` Enum Confusion — `GO_FIRED` vs `TRIGGERED`
**Files:** `scripts/build-bhg-dataset.ts:995`, `src/lib/bhg-engine.ts`, `src/app/api/trades/upcoming/route.ts:158`

The database stores `BhgPhase.GO_FIRED` but the in-memory BHG engine and API use `'TRIGGERED'` as the filter phase:
- `scripts/build-bhg-dataset.ts:995` has `// TODO: rename to 'TRIGGERED' after Prisma schema migration`
- `src/app/api/trades/upcoming/route.ts:158` filters on `s.phase === 'TRIGGERED'`

The two representations need reconciliation. If `TRIGGERED` is an in-memory state only (not in the DB enum), the TODO comment is misleading. If it should be added to the DB enum, a migration is needed.

---

### A-7 (P2): `CANONICAL_SYMBOL_COUNT` Magic Number
**File:** `scripts/ingest-market-prices.ts:20`

```typescript
const CANONICAL_SYMBOL_COUNT = 12
```

This hardcoded count is compared against queried symbol counts to detect unexpected changes. When symbols are added/removed via the registry, this must be manually updated to avoid false alarms or missed detections. It should derive from the registry query result length.

---

## INCOMPLETE / STUB CODE

### I-1 (P1): Market Context and Correlation Alignment Stubs in `/api/trades/upcoming`
**File:** `src/app/api/trades/upcoming/route.ts:91–122`

```typescript
/**
 * Lightweight market context stub for when full context is unavailable.
 * The full buildMarketContext() requires cross-asset candle data which
 * we'll wire in Task 17. For now, provide a minimal context.
 */
function minimalMarketContext(): MarketContext {
  return { regime: 'MIXED', regimeFactors: [], ... }
}

/** Minimal alignment stub — wire full computation in Task 17. */
function minimalAlignment(): CorrelationAlignment {
  return { vix: 0, dxy: 0, nq: 0, composite: 0, isAligned: true, ... }
}
```

These stubs cause:
- `compositeAlignment` to always be `0` in the trade feature vector
- `regime` to always be `'MIXED'` 
- `isAligned` to always be `true`

Since `event` and `correlation` account for 30% of the composite score weight, and both sub-scores use these stub values, the composite scoring is materially incomplete. Trades will appear more aligned and less regime-aware than they actually are.

---

### I-2 (P1): VIX Level Always `null` in Trade Features
**File:** `src/lib/trade-features.ts:518`

```typescript
const vixLvl: number | null = null // BACKTEST-TBD: wire VIX spot from data source
```

`vixLvl` is always `null`. This means:
- `features.vixLevel` is always `null`  
- `features.vixPercentile` is always `0` (the function returns `0` for null input)
- The VIX-triggered risk warning in `trade-reasoning.ts` (`if (features.vixLevel > 30)`) never fires

The VIX data is available in the `econ_vol_indices_1d` table (VIXCLS series). It is not being fetched.

---

### I-3 (P2): All `BACKTEST-TBD` Constants Are Rule-of-Thumb Values
**Files:** `src/lib/event-awareness.ts:61–92`, `src/lib/composite-score.ts:40–48`, `src/lib/trade-features.ts:420`

Significant model parameters are placeholder values not derived from backtest data:
- Event window boundaries (APPROACH, IMMINENT, BLACKOUT, DIGESTING windows)
- Confidence adjustment multipliers per event phase
- Composite score sub-score weights (fib: 0.15, risk: 0.15, event: 0.20, correlation: 0.10, technical: 0.10, mlBaseline: 0.30)
- VIX percentile bucket boundaries

These are explicitly marked `BACKTEST-TBD` throughout the code. The composite score and trade scoring system is producing numbers based on unvalidated rule-of-thumb heuristics, not empirical evidence.

---

### I-4 (P2): `MktFuturesMes4h` and `MktFuturesMes1w` Tables Exist Without Pipelines
**Migration:** `20260227153000_add_mes_4h_1w_tables`

Two new tables were added to the schema but:
- No ingestion script populates them
- No Inngest function schedules their refresh
- No API route queries them
- The `Timeframe.W1` enum value was added in the same migration

These tables are currently empty dead schema.

---

## DATA QUALITY & CONSISTENCY

### D-1 (P1): `SOX.c.0` in Symbol Registry Snapshot
**Files:** `src/lib/symbol-registry/snapshot.ts:450`, `src/lib/symbol-registry/snapshot.json:445`

Note: both `snapshot.ts` (TypeScript export) and `snapshot.json` (raw JSON) are generated files in the registry directory. Both contain `"databentoSymbol": "SOX.c.0"`.

`SOX.c.0` does not exist as a valid Databento continuous contract on GLBX.MDP3. The snapshot (used as fallback when DB is unreachable) still references it as `databentoSymbol: "SOX.c.0"`. Any backfill or ingestion attempt using this symbol from the snapshot will fail silently or with a Databento 422. This was also noted in `docs/audits/validation-feedback-critical-2026-02-27.md` (a prior audit document in this repo) but was never resolved in the snapshot.

**Recommendation:** Update the registry to either remove `SOX` or map it to a valid Databento symbol. Regenerate the snapshot with `npm run registry:snapshot`.

---

### D-2 (P2): Econ Calendar Uses Hardcoded FRED Release IDs
**File:** `src/lib/ingest/econ-calendar.ts`

The econ calendar ingestion uses hardcoded FRED release IDs (e.g., `releaseId: 229`). FRED occasionally renumbers releases. If a release ID changes, the ingestion silently stops fetching dates for that event without any alert.

**Recommendation:** Add an `assertFredReleaseExists()` check on startup, or periodically reconcile release IDs against the FRED API.

---

### D-3 (P2): `z-score` for Economic Surprise Uses Division by `Math.abs(forecast)`
**File:** `src/lib/event-awareness.ts:371`

```typescript
const zScoreProxy = rawDiff / Math.abs(forecast) // BACKTEST-TBD: replace with proper z-score using historical std dev
```

This is not a z-score; it's a relative deviation. Dividing by the absolute value of the forecast value gives a percentage deviation, not a normalized surprise measure. For FOMC, where `forecast` can be near zero (e.g., 0.01% change in fed funds rate), this produces extreme values. A proper z-score requires historical standard deviation of surprises per series.

---

## OPERATIONAL RISKS

### O-1 (P1): `npm install` Fails Without `DIRECT_URL` in Production
**Files:** `package.json:postinstall`, `prisma.config.ts`

The `postinstall` script runs `prisma generate`, which loads `prisma.config.ts`, which calls `env("DIRECT_URL")` in production mode (when `LOCAL_DATABASE_URL` is absent). In CI/CD environments where `DIRECT_URL` is not set, `npm install` fails:

```
PrismaConfigEnvError: Cannot resolve environment variable: DIRECT_URL.
```

Vercel deployments provide `DATABASE_URL` but not necessarily `DIRECT_URL`. If this env var is missing from Vercel's environment, deployment will fail at the `npm install` step.

**Recommendation:** Verify that `DIRECT_URL` is set in all deployment environments, or add a fallback in `prisma.config.ts` that gracefully handles missing `DIRECT_URL` during `prisma generate` (which doesn't actually need a live connection).

---

### O-2 (P1): `/api/live/mes` Polls Databento Every 2 Seconds
**File:** `src/app/api/live/mes/route.ts`

The older SSE endpoint (`/api/live/mes`) polls Databento every 2 seconds via `refreshMes15mFromDatabento`. With a 30-second minimum refresh interval in the refresh function, most calls will be rate-limited, but:
- Each open SSE connection runs `setInterval(2000)` indefinitely
- The `refreshMes15mFromDatabento({ force: false })` still incurs a DB query on every interval even when Databento is throttled
- Multiple concurrent client connections each start their own interval

The newer endpoint (`/api/live/mes15m`) uses 60-second intervals and serves DB data immediately. If the dashboard has migrated to `/api/live/mes15m`, the older route should be deprecated.

---

### O-3 (P1): ML Forecast Requires Manual Python Execution — No Automation
**File:** `src/app/api/ml-forecast/route.ts`

The ML forecast API reads from `public/ml-predictions.json`, which is generated by:
```
.venv-autogluon/bin/python scripts/predict.py --rows=24 --output=public/ml-predictions.json
```

There is no scheduled job, Inngest function, or automation for refreshing this file. If it's stale (>2 hours), the API returns `stale: true`. If the file doesn't exist, the API returns 503. This is a manual operational dependency that has no monitoring or alerting.

---

### O-4 (P2): Module-Level `pg.Pool` Singletons Are Stale on Connection Failure
**Files:** `src/lib/trade-recorder.ts:17`, `src/lib/outcome-tracker.ts:27`

Both files use a module-level `pool` variable:
```typescript
let pool: pg.Pool | null = null
function getPool(): pg.Pool {
  if (!pool) { pool = new pg.Pool(...) }
  return pool
}
```

Once `pool` is set, it's never reset. If the database connection fails (e.g., during a network partition), the existing `pg.Pool` will continue returning errors but the pool is never recreated. In Next.js dev mode (long-running process), this can cause persistent failures that only resolve on server restart.

**Recommendation:** Add pool error event handling with reconnection logic, or use Prisma for these write operations (which already handles reconnection).

---

### O-5 (P2): Prisma Schema Has No `url` in `datasource` Block
**File:** `prisma/schema.prisma:7`

```prisma
datasource db {
  provider = "postgresql"
  // No url field — provided by prisma.config.ts
}
```

The stored memory says `url = env("DATABASE_URL")` is required for tool compatibility, but this schema deliberately omits it, relying on `prisma.config.ts` instead. This works for the three-URL pattern but means `prisma db pull` and third-party tools that read schema directly (without `prisma.config.ts`) will fail with a missing URL error. The `postinstall` failure during `npm install` (O-1) stems from this pattern.

---

## POSITIVE PATTERNS

The following are well-implemented and should be preserved as standards:

✅ **Idempotent ingestion** — All ingestion scripts use `createMany({ skipDuplicates: true })` or upsert patterns. `rowHash` columns prevent duplicates.

✅ **Inngest-driven scheduling** — Scheduled jobs are not cron endpoints but proper Inngest functions with retry policies and step isolation.

✅ **Symbol registry architecture** — The DB-authoritative registry with generated snapshot fallback is the right design. The `getSymbolsByRole()` pattern cleanly separates concerns.

✅ **AI model cascade** — `forecast.ts` and `instant-analysis.ts` implement proper model fallback lists with environment variable overrides. `trade-reasoning.ts` adds a 3-second timeout and deterministic fallback.

✅ **AI probability guardrails** — `trade-reasoning.ts` clamps AI-adjusted probabilities to ±0.20 of the ML baseline, preventing AI hallucinations from overriding quantitative signals.

✅ **`$queryRawUnsafe` allowlist pattern** — `_staleness-audit.ts` and `_check-econ-tables.ts` both correctly implement allowlist validation before raw SQL. This pattern should be applied consistently (see S-3).

✅ **UTC-only timestamps** — All timestamp columns use `@db.Timestamptz(6)`. No local time storage found.

✅ **Three-URL database pattern** — Clean separation of Accelerate (production), direct (scripts/migrations), and local (dev) URLs with explicit routing logic in `db-url.ts`.

✅ **Feature lookahead prevention** — Dataset builders use `< tsKey` for rolling windows (not `<=`) to prevent lookahead bias.

✅ **Error logging patterns** — `console.warn()` for non-fatal / fire-and-forget failures, `console.error()` for unexpected route errors.

---

## PRIORITY SUMMARY

| ID | Severity | Area | Issue |
|----|----------|------|-------|
| S-1 | **P0** | Security | No authentication on any API route |
| S-2 | **P1** | Security | No image size/content validation on `/api/analyse/chart` |
| S-3 | **P1** | Security | `$queryRawUnsafe` without allowlist in build scripts |
| I-1 | **P1** | Incomplete | Market context and alignment stubs in `/api/trades/upcoming` |
| I-2 | **P1** | Incomplete | VIX level always `null` in trade feature vector |
| A-1 | **P1** | Architecture | Hardcoded symbol list in `backfill-futures-all.ts` |
| A-3 | **P1** | Architecture | Hardcoded model in `/api/analyse/chart` — no fallback |
| D-1 | **P1** | Data | `SOX.c.0` invalid in symbol registry snapshot |
| O-1 | **P1** | Operational | `npm install` fails without `DIRECT_URL` |
| O-2 | **P1** | Operational | `/api/live/mes` polls every 2s (legacy endpoint) |
| O-3 | **P1** | Operational | ML forecast has no automation — manual Python required |
| S-4 | **P2** | Security | AI JSON parsed without schema validation |
| A-2 | **P2** | Architecture | `EXPECTED_SYMBOLS` hardcoded in correlation route |
| A-4 | **P2** | Architecture | `rowToCandle` copy-pasted across 7+ files |
| A-5 | **P2** | Architecture | `aggregateCandles` duplicated in route files |
| A-6 | **P2** | Architecture | `GO_FIRED` vs `TRIGGERED` phase confusion |
| A-7 | **P2** | Architecture | `CANONICAL_SYMBOL_COUNT` magic number |
| I-3 | **P2** | Incomplete | All `BACKTEST-TBD` weights are rule-of-thumb only |
| I-4 | **P2** | Incomplete | MktFuturesMes4h/1w tables have no pipelines |
| D-2 | **P2** | Data | Hardcoded FRED release IDs in econ-calendar |
| D-3 | **P2** | Data | Economic surprise uses division-by-forecast not z-score |
| O-4 | **P2** | Operational | `pg.Pool` singletons don't recover from connection failure |
| O-5 | **P2** | Operational | Prisma schema missing `url` causes tooling friction |

---

*Audit conducted: 2026-03-01*  
*Base commit: c534095 (fix(backfill): fail on unrecoverable Databento 422/timeouts)*
